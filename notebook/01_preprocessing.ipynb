{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e06a464-e721-4cfe-bd9b-c7bc4b666600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564e3494-a443-43d5-8a5b-7cea99c8bcaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in c:\\users\\tomi\\anaconda3\\lib\\site-packages (20250625)\n",
      "Requirement already satisfied: pypub in c:\\users\\tomi\\anaconda3\\lib\\site-packages (1.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\tomi\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from openai-whisper) (10.3.0)\n",
      "Requirement already satisfied: numba in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from openai-whisper) (0.61.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from openai-whisper) (2.1.3)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from openai-whisper) (0.11.0)\n",
      "Requirement already satisfied: torch in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from openai-whisper) (2.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pypub) (4.12.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pypub) (3.1.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pypub) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from beautifulsoup4->pypub) (2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from jinja2->pypub) (3.0.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from numba->openai-whisper) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from requests->pypub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from requests->pypub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from requests->pypub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from requests->pypub) (2025.8.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai-whisper pypub pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2541f6af-12e4-4fb5-b70c-b297600c400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-python in c:\\users\\tomi\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7ce8fd-0414-4fe1-81f6-62d849810bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba11f862-8c87-4605-b16b-809b8df30388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da32e291-e79d-456b-b525-0362a18b89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = r\"C:\\Users\\TOMI\\Documents\\callcentre_profiling\\audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ea1ca8-f277-4528-b434-37cfeb99af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f2f9f66-97de-437f-8fa8-fea2178f2109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing call_recording_01.wav, 1\n",
      "Transcribing call_recording_02.wav, 2\n",
      "Transcribing call_recording_03.wav, 3\n",
      "Transcribing call_recording_04.wav, 4\n",
      "Transcribing call_recording_05.wav, 5\n",
      "Transcribing call_recording_06.wav, 6\n",
      "Transcribing call_recording_07.wav, 7\n",
      "Transcribing call_recording_08.wav, 8\n",
      "Transcribing call_recording_09.wav, 9\n",
      "Transcribing call_recording_10.wav, 10\n",
      "Transcribing call_recording_11.wav, 11\n",
      "Transcribing call_recording_12.wav, 12\n",
      "Transcribing call_recording_13.wav, 13\n",
      "Transcribing call_recording_14.wav, 14\n",
      "Transcribing call_recording_15.wav, 15\n",
      "Transcribing call_recording_16.wav, 16\n",
      "Transcribing call_recording_17.wav, 17\n",
      "Transcribing call_recording_18.wav, 18\n",
      "Transcribing call_recording_19.wav, 19\n",
      "Transcribing call_recording_20.wav, 20\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        count +=1\n",
    "        filepath = os.path.join(audio_dir, filename)\n",
    "        print(f\"Transcribing {filename}, {count}\")\n",
    "\n",
    "        result = model.transcribe(filepath, fp16=False)\n",
    "        text = result['text']\n",
    "\n",
    "        output_data.append({\"file name\": filename, \"transcript\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dd9e3d6-a625-407f-9b78-72cf260e4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dab847c-3804-455c-b8c2-f7621074f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(output_data)\n",
    "df.to_csv(\"transcripted.csv\", index=False)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093f5433-ae47-456b-8cda-cee372122b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\tomi\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tomi\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: regex in c:\\users\\tomi\\anaconda3\\lib\\site-packages (2024.11.6)\n",
      "Requirement already satisfied: unidecode in c:\\users\\tomi\\anaconda3\\lib\\site-packages (1.3.8)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\tomi\\anaconda3\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\tomi\\anaconda3\\lib\\site-packages (19.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy regex unidecode rapidfuzz pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cdc3891-eeea-4d4c-a5cc-f5c2dbf39384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9effa3e-00e8-4e67-943d-e5123cdac949",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Download NLTK resources (only first time)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d4391df-bfcd-4841-baac-36424c308203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV\n",
    "df = pd.read_csv(\"transcripted.csv\")\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation & numbers\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e859b94b-68ec-41ae-9655-d90d57756a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete. Saved as transcript_cleaned.csv\n",
      "               file name                                            cleaned\n",
      "0  call_recording_01.wav  hello im sarah miller im calling inquire ac ai...\n",
      "1  call_recording_02.wav  extremely dissatisfied recent order john davis...\n",
      "2  call_recording_03.wav  hi maria rodriguez im trouble lap laptop order...\n",
      "3  call_recording_04.wav  wanted call say pleased dw dishwasher purchase...\n",
      "4  call_recording_05.wav  hello name jessica brown id like place order w...\n"
     ]
    }
   ],
   "source": [
    "df[\"cleaned\"] = df[\"transcript\"].apply(clean_text)\n",
    "df.to_csv(\"transcript_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"✅ Preprocessing complete. Saved as transcript_cleaned.csv\")\n",
    "print(df[['file name', 'cleaned']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb67dd7a-231f-48bb-892f-7804a12ff1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file saved: customer_profile.csv\n",
      "               file name                                         transcript  \\\n",
      "0  call_recording_01.wav   Hello, I'm Sarah Miller. I'm calling to inqui...   \n",
      "1  call_recording_02.wav   I am extremely dissatisfied with my recent or...   \n",
      "2  call_recording_03.wav   Hi, this is Maria Rodriguez. I'm having troub...   \n",
      "3  call_recording_04.wav   I just wanted to call and say how pleased I a...   \n",
      "4  call_recording_05.wav   Hello, my name is Jessica Brown. I'd like to ...   \n",
      "\n",
      "                                             cleaned sentiment  \n",
      "0  hello im sarah miller im calling inquire ac ai...            \n",
      "1  extremely dissatisfied recent order john davis...            \n",
      "2  hi maria rodriguez im trouble lap laptop order...            \n",
      "3  wanted call say pleased dw dishwasher purchase...            \n",
      "4  hello name jessica brown id like place order w...            \n"
     ]
    }
   ],
   "source": [
    "# Load your transcribed dataset\n",
    "df = pd.read_csv(\"transcript_cleaned.csv\")\n",
    "\n",
    "# ✅ Add new column with empty strings\n",
    "df[\"sentiment\"] = \"\"\n",
    "\n",
    "# ✅ Save it back to CSV\n",
    "df.to_csv(\"customer_profile.csv\", index=False)\n",
    "\n",
    "print(\"New file saved: customer_profile.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ffadb7a-e6e2-44a8-bf73-afe848883d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: C:\\Users\\TOMI\\Documents\\callcentre_profiling\\notebook\n",
      "✅ Preprocessed file saved at: C:\\Users\\TOMI\\Documents\\callcentre_profiling\\notebook\\customer_profile.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check current working directory\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "# Save file to this directory\n",
    "output_path = os.path.join(os.getcwd(), \"customer_profile.csv\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Preprocessed file saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "931e8a98-6be4-4a5d-8814-67fc3b370d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\tomi\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.55.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tomi\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aba557fc-6bcd-475d-9572-a2f1f90c3502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d0c9c80e05416ea1e6ba31cfb9650f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings shape: (20, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv(\"transcript_cleaned.csv\")\n",
    "\n",
    "# Load sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(df['cleaned'], show_progress_bar=True)\n",
    "\n",
    "print(\"✅ Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff2b91-d955-43b3-bb3a-4fc387dac602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your preprocessed CSV\n",
    "df = pd.read_csv(\"transcript_cleaned.csv\")  # Update with your file\n",
    "\n",
    "# Define 5 personality categories\n",
    "categories = [\"short-tempered\", \"cooperative\", \"neutral\", \"reserved\", \"friendly\"]\n",
    "\n",
    "# Randomly assign a category to each transcript\n",
    "np.random.seed(42)  # For reproducibility\n",
    "df['personality'] = np.random.choice(categories, size=len(df))\n",
    "\n",
    "# Save the updated file\n",
    "df.to_csv(\"conversation_with_labels.csv\", index=False)\n",
    "\n",
    "print(\"✅ Added personality labels. Here's a preview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ab5507f-2d11-4293-b4f2-e08a55b026ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 0.23684210526315788\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   cooperative       0.36      0.57      0.44         7\n",
      "      friendly       0.00      0.00      0.00         7\n",
      "       neutral       0.17      0.14      0.15         7\n",
      "      reserved       0.22      0.22      0.22         9\n",
      "short-tempered       0.25      0.25      0.25         8\n",
      "\n",
      "      accuracy                           0.24        38\n",
      "     macro avg       0.20      0.24      0.21        38\n",
      "  weighted avg       0.20      0.24      0.22        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Load labeled dataset\n",
    "df = pd.read_csv(\"conversation_with_labels.csv\")\n",
    "\n",
    "# 2. Features (text) & Labels (personality)\n",
    "X = df['cleaned_text']\n",
    "y = df['personality']\n",
    "\n",
    "# 3. Split into Train & Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 4. Convert text → numerical features (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 5. Train classifier (Logistic Regression)\n",
    "clf = LogisticRegression(max_iter=200, class_weight=\"balanced\")\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# 6. Evaluate\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58c3ebf4-98da-4b8d-9762-1dd1de56920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model & Vectorizer saved!\n",
      "Predicted personality: reserved\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model and vectorizer\n",
    "joblib.dump(clf, \"personality_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "print(\"✅ Model & Vectorizer saved!\")\n",
    "\n",
    "# Load them later\n",
    "clf_loaded = joblib.load(\"personality_model.pkl\")\n",
    "vectorizer_loaded = joblib.load(\"vectorizer.pkl\")\n",
    "\n",
    "# Example prediction\n",
    "sample_text = [\"I need help with my account, it's really frustrating!\"]\n",
    "sample_vec = vectorizer_loaded.transform(sample_text)\n",
    "print(\"Predicted personality:\", clf_loaded.predict(sample_vec)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "791a0073-4b79-43cf-996e-056a129a060e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a63c2feb264fd1985a2be84fa74604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings shape: (186, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset with labels\n",
    "df = pd.read_csv(\"conversation_with_labels.csv\")\n",
    "\n",
    "# Load pretrained sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode cleaned_text column into embeddings\n",
    "embeddings = model.encode(df[\"cleaned_text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(\"✅ Embeddings shape:\", embeddings.shape)  # (num_samples, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3647d481-b995-460d-9590-7e01b9412f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 0.15789473684210525\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   cooperative       0.17      0.17      0.17         6\n",
      "      friendly       0.00      0.00      0.00         6\n",
      "       neutral       0.20      0.17      0.18         6\n",
      "      reserved       0.24      0.40      0.30        10\n",
      "short-tempered       0.00      0.00      0.00        10\n",
      "\n",
      "      accuracy                           0.16        38\n",
      "     macro avg       0.12      0.15      0.13        38\n",
      "  weighted avg       0.12      0.16      0.13        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, df[\"personality\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75cede06-31bb-4514-a938-5788e4acd163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ready for manual labeling: new_conversation_for_labeling.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"conversation_with_labels.csv\")\n",
    "\n",
    "# Remove old random labels if any\n",
    "if \"personality\" in df.columns:\n",
    "    df = df.drop(columns=[\"personality\"])\n",
    "\n",
    "# Add empty personality column\n",
    "df[\"personality\"] = \"\"\n",
    "\n",
    "df.to_csv(\"new_conversation_for_labeling.csv\", index=False)\n",
    "print(\"✅ Ready for manual labeling: new_conversation_for_labeling.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee17130-3c57-4068-97bc-74dfcc9a4cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
